---
title: "Exercises on Generalized Linear Models"
output: html_document
---

# Problem 1: Students with an infectious disease

Suppose we are interested in the number of high school students diagnosed with an infectious disease as a function of the number of days from the initial outbreak. The data can be loaded as follows:

```{r}
students = read.csv("data/students.csv")
head(students)
```

Can you find a suitable model for this dataset?

### Solution

We can first visualize the data as follows:

```{r}
plot(students$day, students$cases, xlim = c(0,120), ylim = c(0,12),
     xlab = "Days since initial outbreak",
     ylab = "Number of diagnosed students",
     pch = 16, col = "#F8766D99")
grid()
```

As we are modeling count data, we can use a Poisson regression. There are two functions in R that can fit a Poisson regression. The first one is the classical `glm()` function by specifying `family=poisson`.

```{r}
fit.glm = glm(cases ~ day, data = students, family = poisson)
summary(fit.glm)
```

In this case, we can perform the model check as follows:

```{r, fig.align='center', fig.height=5, fig.width=6}
par(mfrow = c(2,2))
plot(fit.glm)
```

Alternatively, you can use the `gamlss()` function in the `gamlss` R package by specifying `family = PO`.

```{r, warning=FALSE, message=FALSE}
library(gamlss)
```

```{r}
fit.gamlss = gamlss(cases ~ day, data = students, family = PO)
summary(fit.gamlss)
```

The `gamlss` approach provides a nicer summary to assess how well your model fit the data at hands:

```{r, fig.align='center', fig.height=5, fig.width=6}
par(mfrow = c(1,1))
plot(fit.gamlss)
```

# Problem 2: Pima Indians Diabetes

Consider a dataset on 768 women of at least 21 years old of the Pima Indian heritage. This dataset includes the following variables:

- `pregnant`: Number of times pregnant
- `glucose`: Plasma glucose concentration in an oral glucose tolerance test
- `pressure`: Diastolic blood pressure (mm Hg)
- `triceps`: Triceps skin fold thickness (mm)
- `insulin`: 2-Hour serum insulin (mu U/ml)
- `mass`: Body mass index (weight in kg/(height in m)^2)
- `pedigree`: Diabetes pedigree function
- `age`: Age of the patients (years)
- `diabetes`: Class variable (test for diabetes)

The dataset is stored in the `mlbench` R package and you can load it as follows:

```{r}
# install.packages("mlbench")
library(mlbench)
data(PimaIndiansDiabetes)
head(PimaIndiansDiabetes)
```

Can you find a suitable model to predict the possibility of a woman (Pima Indian heritage) being diagnosed with diabetes based on these variables (or a subset of them)?

### Solution

As we are interested in fitting binary data (positive or negative of diabetes), we consider to fit a logistic regression approach. We start by fitting an initial model with all covariates included (without interactions):

```{r}
fit.glm = glm(diabetes ~ ., data = PimaIndiansDiabetes, family = binomial(link="logit"))
summary(fit.glm)
```

We can see that among these 8 variables, some appear not significant, implying that we may be able to find a smaller model with less variables. As an example, we can use the `step()` function to perform stepwise model selection using the AIC by removing variables iteratively from our initial model. This approach is known as "stepwise backward AIC" and it is an heuristic method which avoids to explore ALL models. Indeed, we have in this case $2^8 = 256$ models without interactions and $2^{36} = 68,719,476,736$ with first order interactions. If fitting one models takes 1 second, we would need over 2000 years to fit all models. Nevertheless, various modern methods allow to improve on this stepwise approach but they are beyond the scope of this class.

```{r}
fit.glm.aic = step(fit.glm, trace = FALSE)
summary(fit.glm.aic)
```

We now find a model with 7 variables (i.e. remove `triceps`) with slightly smaller AIC.

```{r}
AIC(fit.glm)
AIC(fit.glm.aic)
```

It is also possible to consider a forward approach starting from a simple model. This can be done as follows:

```{r}
# Initial model
fit.glm.inital = glm(diabetes ~ 1, data = PimaIndiansDiabetes, 
                     family = binomial(link="logit"))

fit.glm.aic.forward = step(fit.glm.inital, 
                           scope=list(lower = formula(fit.glm.inital),
                              upper=formula(fit.glm)), 
                   direction="forward", trace = FALSE)
```

In this case, the two methods (i.e. forward and backward) are equivalent: 

```{r}
AIC(fit.glm.aic)
AIC(fit.glm.aic.forward)
summary(fit.glm.aic.forward)
```

Now let's check how reliable our model is. If the predicted probability is larger than 0.5, then we predict that the considered individual will have diabetes (so positive). Otherwise we predict that the individual is negative. Then we can compute the in-sample classification accuracy by comparing the predicted values to the actual observed values for the whole sample. 

```{r}
class_predict = fit.glm.aic$fitted.values > 0.5
in_accuracy = mean((PimaIndiansDiabetes$diabetes == "pos") == class_predict)
in_accuracy
```

In this case, we have `r round(in_accuracy*100,2)`% in-sample classification accuracy. Is that high? 

```{r}
table(PimaIndiansDiabetes$diabetes)/768
```

So the in-sample classification accuracy is actually higher than if we blindly predict individuals to be positive or negative. Therefore, our model is working properly. 

Now let's consider the out-of-sample classification accuracy. 

```{r, warning=FALSE, message=FALSE}
library(boot)
```

```{r}
cost = function(resp, pred){
  mean(resp == (pred > 0.5))
}
out_accuracy = cv.glm(PimaIndiansDiabetes, fit.glm.aic, cost, K = 10)$delta[2]
out_accuracy
```

In this case, we have `r round(out_accuracy*100,2)`% out-of-sample classification accuracy, which is very similar to the in-sample classification accuracy. This is because we have a large number of observations ($n=768$) compared to the number of parameters to estimate ($p=8$). So we again verify that our model is working properly.

Lastly, we can use the `gamlss()` function to perform the model check:

```{r}
fit.gamlss = gamlss(formula(fit.glm.aic), data=PimaIndiansDiabetes, family=BI)
plot(fit.gamlss)
```



# Problem 3: Breast Cancer

Let's consider a breast cancer dataset from the `mlbench` R package. In this example, the objective is to predict whether a cancer is malignant or benign from biopsy details. This dataset includes 699 observations on 11 variables. You can load the data as follows:

```{r}
data(BreastCancer)
dim(BreastCancer)
head(BreastCancer)
table(BreastCancer$Class)
```

More information about this dataset can be found by typing `?BreastCancer` in R. Can you find a suitable model to identify benign or malignant classes based on these variables (or a subset of them)?

### Solution

We notice that there are some missing values in the dataset. Therefore, we start by removing the observations with missing data.

```{r}
BreastCancer = na.omit(BreastCancer)
dim(BreastCancer)
```

After removing the missing values, we remain to have 683 observations. We start our analysis by fitting an initial model with all covariates included:

```{r}
# Initial model
fit.glm = glm(Class ~ Cl.thickness + Cell.size + Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli + Mitoses, data = BreastCancer, family=binomial)
summary(fit.glm)
```

As we can see from the output, it looks like there's a problem with this initial model. Why is that?

To fit a logistic regression, a quantity called the Events Per Variable (EPV) can give us an idea of the largest model we can consider given the number of observations. The EPV is defined as the number of occurrences of the least frequent outcome over the number of covariates. An EPV of 10 is a widely advocated minimal criterion for sample size considerations in logistic regression analysis. However, there's a lack of evidence to support this EPV rule of thumb. In fact, some studies have pointed out the urgent need for new research to provide more valid guidance. More details can for example be found [here](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-016-0267-3).

```{r}
n = min(table(BreastCancer$Class))
p = 24 # assumed number of paprameter
EPV = n/p

length(fit.glm$coefficients) # actual number of parameters in the initial model
```

In our case, in order to have an EPV of approximately 10, we should consider up to 24 parameters, but our initial model has 81 parameters! Therefore, this model is not valid. Let's find a more appropriate model using a forward approach based on the AIC starting from a simple model.

```{r}
# Initial model
fit.glm.inital = glm(Class ~ 1, data = BreastCancer,family=binomial)
forward.fit = step(fit.glm.inital, 
                   scope=list(lower = formula(fit.glm.inital),
                              upper=formula(fit.glm)), 
                   direction="forward", trace = FALSE)

summary(forward.fit)
length(forward.fit$coefficients)
```

As we can see, using this forward approach based on the AIC, we find a model of 46 parameters, which is still larger than the expected 24 and therefore the model fails. Alternatively, we can use a forward approach based on the BIC to impose more penalties on the covariates:

```{r}
forward.fit.BIC = step(fit.glm.inital, 
                   scope=list(lower = formula(fit.glm.inital),
                              upper=formula(fit.glm)), 
                   direction="forward", k = log(length(BreastCancer$Id)), 
                   trace = FALSE)

summary(forward.fit.BIC)
length(forward.fit.BIC$coefficients) # Better!
```

With BIC we obtain a model with only 19 parameters, which is smaller than 24! Now let's check how reliable this model is. We can again compute the in-sample classification accuracy and the out-of-sample classifictaion accuracy.

```{r}
class_predict = forward.fit.BIC$fitted.values > 0.5
in_accuracy = mean((BreastCancer$Class == "malignant") == class_predict)
in_accuracy
```

```{r}
table(BreastCancer$Class)/length(BreastCancer$Class)
```

So the in-sample classification accuracy is actually higher than if we blindly identifying the benign or malignant classes. Therefore, our model is working properly. 

Now let's consider the out-of-sample classification accuracy. 

```{r, warning=FALSE, message=FALSE}
out_accuracy = cv.glm(BreastCancer, forward.fit.BIC, cost, K = 10)$delta[2]
out_accuracy
```

The resulting out-of-sample classification accuracy is very similar to the in-sample classification accuracy, which again verify that our model is working properly. 

Lastly, we can again use the `gamlss()` function to perform the model check:

```{r}
fit.gamlss = gamlss(formula(forward.fit.BIC), data = BreastCancer, family=BI)
summary(fit.gamlss)
plot(fit.gamlss)
```
