<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Modelling and Data Analytics for Pharmaceutical Sciences</title>
    <meta charset="utf-8" />
    <meta name="author" content="St√©phane Guerrier" />
    <link href="Lecture2_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="Lecture2_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="Lecture2_files/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link href="Lecture2_files/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="Lecture2_files/tile-view-0.2.6/tile-view.js"></script>
    <link href="Lecture2_files/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="Lecture2_files/panelset-0.2.6/panelset.js"></script>
    <script src="Lecture2_files/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="Lecture2_files/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="Lecture2_files/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link href="Lecture2_files/xaringanExtra-extra-styles-0.2.6/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">










class: title-slide  
&lt;div class="my-logo-right"&gt;&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
 
# Data Analytics for Pharmaceutical Sciences

## Part II: Analysis of Variance (ANOVA)

### .smaller[St√©phane Guerrier, Data Analytics Lab, University of Geneva, üá®üá≠]
### .smaller[Dominique-L. Couturier, Cancer Research UK, University of Cambridge, üá¨üáß]

&lt;br&gt;
&lt;br&gt;
&lt;img src="pics/liscence.png" width="25%" style="display: block; margin: auto;" /&gt;
.center[.tiny[License: [CC BY NC SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)]]

### .tiny[This document was prepared with the help of Wenfei Chu &amp; Yuming Zhang]

---


# .smaller[Two-sample Location Tests]

In practice, we often encounter problems where our goal is .pink[to compare the means (or locations) of two samples]. For example,
1. A scientist is interested in comparing the vaccine efficacy of the Pfizer-BioNTech and the Moderna vaccine.
2. A bank wants to know which of two proposed plans will most increase the use of its credit cards.
3. A psychologist wants to compare male and female college students' impression on a selected webpage.

We will discuss three ".pink[two-sample location tests]":
1. .purple[Two independent sample Student's t-test]
2. .purple[Two independent sample Welch's t-test]
3. .purple[Two independent sample Mann-Whitney-Wilcoxon test]

---

# .smaller[Two Independent Sample Student's t-test]

This test considers the following assumed model for group .hi-purple2[A] and .hi-blue[B]

`$$X_{i(g)} = \color{#eb078e}{\mu_{g}} + \varepsilon_{i(g)} = \mu + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},$$`
where `\(g=A,B\)`, `\(i=1,...,n_{g}\)`, `\(\varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma^{2}})\)` and `\(\sum n_{g}\delta_{g} = \color{#6A5ACD}{\delta_A} + \color{#06bcf1}{\delta_B}=0\)`. 

üìù `\(\color{#6A5ACD}{n_A}\)` `\(=\)` sample size of group .hi-purple2[A], `\(\color{#6A5ACD}{\mu_{A} = \mu + \delta_A}\)` `\(=\)` population mean of group .purple2[A], `\(\color{#06bcf1}{n_B}\)` and `\(\color{#06bcf1}{\mu_{B} = \mu + \delta_B}\)` are similarly defined for group .hi-blue[B].

Hypotheses:

`$$H_0: \color{#6A5ACD}{\mu_A} - \color{#06bcf1}{\mu_B} \color{#eb078e}{=} \mu_0 \ \ \ \ \text{and} \ \ \ \ H_a: \color{#6A5ACD}{\mu_A} - \color{#06bcf1}{\mu_B} \ \big[ \color{#eb078e}{&gt;} \text{ or } \color{#eb078e}{&lt;} \text{ or } \color{#eb078e}{\neq} \big] \ \mu_0.$$`

Test statistic's distribution under `\(H_0\)`

`$$\color{#b4b4b4}{T = \frac{(\overline{X}_{A}-\overline{X}_{B})-\mu_0}{s_{p}\sqrt{n_{A}^{-1}+n_{B}^{-1}}} \ {\underset{H_0}{\sim}} \ \text{Student}(n_{A}+n_{B}-2).}$$` 
---

# .smaller[Discussion - Student's t-test]

- R function: 

.center[
`t.test(x = ..., y = ..., alternative = ..., var.equal = TRUE)`.]

- This test strongly relies on the .pink[assumed absence of outliers]. If outliers appear to be present the Mann-Whitney-Wilcoxon Test (see later) is (probably) a better option.
- For moderate and small sample sizes, the sample distribution should be at least .pink[approximately normal] with no strong skewness to ensure the reliability of the test.
- In practice, the assumption of equal variance is hard to verify and .hi.pink[we recommend avoiding this test in practice].

---

# .smaller[Two Independent Sample Welch's t-test]

This test considers the following assumed model for group .hi-purple2[A] and .hi-blue[B]

`$$X_{i(g)} = \color{#eb078e}{\mu_{g}} + \varepsilon_{i(g)} = \mu + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},$$`
where `\(g=A,B\)`, `\(i=1,...,n_{g}\)`, `\(\varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma^{2}_g})\)` and `\(\sum n_{g}\delta_{g} = \color{#6A5ACD}{\delta_A} + \color{#06bcf1}{\delta_B}=0\)`. 

üìù `\(\color{#6A5ACD}{n_A}\)` `\(=\)` sample size of group .hi-purple2[A], `\(\color{#6A5ACD}{\mu_{A} = \mu + \delta_A}\)` `\(=\)` population mean of group .purple2[A], `\(\color{#06bcf1}{n_B}\)` and `\(\color{#06bcf1}{\mu_{B} = \mu + \delta_B}\)` are similarly defined for group .hi-blue[B].

Hypotheses: 

`$$H_0: \color{#6A5ACD}{\mu_A} - \color{#06bcf1}{\mu_B} \color{#eb078e}{=} \mu_0 \ \ \ \ \text{and} \ \ \ \ H_a: \color{#6A5ACD}{\mu_A} - \color{#06bcf1}{\mu_B} \ \big[ \color{#eb078e}{&gt;} \text{ or } \color{#eb078e}{&lt;} \text{ or } \color{#eb078e}{\neq} \big] \ \mu_0.$$`

Test statistic's distribution under `\(H_0\)`

`$$\color{#b4b4b4}{T = \frac{(\overline{X}_{A}-\overline{X}_{B})-\mu_0}{\sqrt{s^2_A/n_{A} + s^2_B/n_{B}}} \ {\underset{H_0}{\sim}} \ \text{Student}(df).}$$` 

---

# .smaller[Discussion - Welch's t-test]

- R function: 

.center[
`t.test(x = ..., y = ..., alternative = ...)`.]

- This test strongly relies on the .pink[assumed absence of outliers]. If outliers appear to be present the Mann-Whitney-Wilcoxon Test (see later) is (probably) a better option.
- For moderate and small sample sizes, the sample distribution should be at least .pink[approximately normal] with no strong skewness to ensure the reliability of the test.
- This test does not require the variances of the two groups to be equal. If the variances of the two groups are the same (which is rather unlikely in practice), the Welch's t-test losses a little bit of power compared to the Student's t-test.
- The computation of `\(df\)` (i.e. the degrees of freedom of the distribution under the null) is beyond the scope of this class.

---

# .smaller[Mann-Whitney-Wilcoxon Test]

This test considers the following assumed model for group .hi-purple2[A] and .hi-blue[B]

`$$X_{i(g)} = \color{#eb078e}{\theta_{g}} + \varepsilon_{i(g)} = \theta + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},$$`
where `\(g=A,B\)`, `\(i=1,...,n_{g}\)`, `\(\varepsilon_{i(g)} \overset{iid}{\sim} (0,\color{#eb078e}{\sigma^{2}})\)` and `\(\sum n_{g}\delta_{g} = \color{#6A5ACD}{\delta_A} + \color{#06bcf1}{\delta_B}=0\)`. 

üìù `\(\color{#6A5ACD}{n_A}\)` `\(=\)` sample size of group .hi-purple2[A], `\(\color{#6A5ACD}{\theta_{A} = \theta + \delta_A}\)` `\(=\)` population .pink[location] of group .purple2[A], `\(\color{#06bcf1}{n_B}\)` and `\(\color{#06bcf1}{\theta_{B} = \theta + \delta_B}\)` are similarly defined for group .hi-blue[B].

Hypotheses: `\(H_0: \color{#6A5ACD}{\theta_A} - \color{#06bcf1}{\theta_B} \color{#eb078e}{=} \theta_0 \ \ \ \ \text{and} \ \ \ \ H_a: \color{#6A5ACD}{\theta_A} - \color{#06bcf1}{\theta_B} \ \big[ \color{#eb078e}{&gt;} \text{ or } \color{#eb078e}{&lt;} \text{ or } \color{#eb078e}{\neq} \big] \ \theta_0.\)`

Test statistic's distribution under `\(H_0\)`

`$$\color{#b4b4b4}{Z = \frac{\sum_{i=1}^{n_{B}}R_{i(g)}-[n_{B}(n_{A}+n_{B}+1)/2]}{\sqrt{n_{A}n_{B}(n_{A}+n_{B}+1)/12}},}$$`
.grey[where] `\(\color{#b4b4b4}{R_{i(g)}}\)` .grey[denotes the global rank of the] `\(\color{#b4b4b4}{i}\)`.grey[-th observation of group] `\(\color{#b4b4b4}{g}\)`.grey[.]

---

# .smaller[Discussion - Mann-Whitney-Wilcoxon test]

- R function: 

.center[
`wilcox.test(x = ..., y = ..., alternative = ...)`.]

- This test is ".pink[robust]" in the sense that (unlike the t-test) it is not overly affected by outliers.
- The Mann-Whitney-Wilcoxon test does not require the distributions to be Gaussian nor the variances to be the same, however, in this case this method actually tests whether the two distributions are the same. To render the Mann-Whitney-Wilcoxon test comparable to the t-tests we need to assume: (1) The distribution is symmetric, (2) the variances are the same. Then, we have `\(\color{#6A5ACD}{\theta_A = \mu_A}\)` and `\(\color{#06bcf1}{\theta_B = \mu_B}\)`.
- Compared to the t-tests, the Mann-Whitney-Wilcoxon test is less power if their requirements (Gaussian and possibly same variances) are met.
- The distribution of this method under the null is complicated and can be obtained by different methods (e.g. exact, asymptotic normal, ...). The details are beyond the scope of this class.
 
---

# Comparing Diet A and B 

.panelset[
.panel[.panel-name[Graph]
&lt;img src="pics/dietAB.png" width="80%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Import]

```r
# Import data
diet = read.csv("data/diet.csv",row.names=1)

# Compute weight loss
diet$weight.loss = diet$initial.weight - diet$final.weight

# Select diet
posA = diet$diet.type=="A"
posB = diet$diet.type=="B"

# Variable of interest
dietA = diet$weight.loss[posA]
dietB = diet$weight.loss[posB]
```
]
.panel[.panel-name[Student]

```r
t.test(dietA, dietB, var.equal = TRUE)
```

```
#&gt; 
#&gt; 	Two Sample t-test
#&gt; 
#&gt; data:  dietA and dietB
#&gt; t = 0.0475, df = 47, p-value = 0.9623
#&gt; alternative hypothesis: true difference in means is not equal to 0
#&gt; 95 percent confidence interval:
#&gt;  -1.323275  1.387275
#&gt; sample estimates:
#&gt; mean of x mean of y 
#&gt;     3.300     3.268
```
]
.panel[.panel-name[Welsh]

```r
t.test(dietA, dietB)
```

```
#&gt; 
#&gt; 	Welch Two Sample t-test
#&gt; 
#&gt; data:  dietA and dietB
#&gt; t = 0.047594, df = 46.865, p-value = 0.9622
#&gt; alternative hypothesis: true difference in means is not equal to 0
#&gt; 95 percent confidence interval:
#&gt;  -1.320692  1.384692
#&gt; sample estimates:
#&gt; mean of x mean of y 
#&gt;     3.300     3.268
```
]
.panel[.panel-name[Wilcox]

```r
wilcox.test(dietA, dietB)
```

```
#&gt; 
#&gt; 	Wilcoxon rank sum test with continuity correction
#&gt; 
#&gt; data:  dietA and dietB
#&gt; W = 277, p-value = 0.6526
#&gt; alternative hypothesis: true location shift is not equal to 0
```
]
.panel[.panel-name[Results]
1. .purple[Define hypotheses:] `\(H_0: \mu_A = \mu_B\)` and `\(H_a: \mu_A \color{#e64173}{\neq} \mu_B\)`.
2. .purple[Define] `\(\color{#373895}{\alpha}\)`: We consider `\(\alpha = 5\%\)`.
3. .purple[Compute p-value]: Welch's t-test appears suitable in this case, therefore, we obtain: p-value =  `\(96.22 \%\)` (see R output tab for details).
4. .purple[Conclusion:] We have p-value &gt; `\(\alpha\)` and we fail to reject the null hypothesis at the significance level of 5%.
]
]

---


# Problems with multiple samples

In practice, we often even encounter situations where we need to .pink[compare the means of more than 2 groups]. For example,

1. A scientist wants to compare the vaccine efficacy of the Pfizer-BioNTech, the Moderna, and the Sputnik V vaccines.
2. A nutritionist wants to compare the weight loss efficacy of several diets. 

`\(\mu_A \approx \mu_B &lt; \mu_C\)`


---

# Jelly beans example

.purple[Are jelly beans causing acne? Maybe... but why only green ones?] ü§® 

&lt;img src="pics/green.png" width="45%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]
---

# Are jelly beans causing acne?

&lt;br&gt;
&lt;img src="pics/green1.png" width="85%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]

---

# Maybe a specific color?

&lt;br&gt;
&lt;img src="pics/green2.png" width="76%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]

---

# Maybe a specific color?

&lt;br&gt;
&lt;img src="pics/green3.png" width="75%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]

---

# And finally...

&lt;img src="pics/green.png" width="45%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]

üëã .smallest[If you want to know more about this comics have a look [here](https://www.explainxkcd.com/wiki/index.php/882:_Significant).]

---

# ‚ò†Ô∏è Multiple testing can be dangerous!

- .smaller[Remember that a p-value is .hi-pink[random] as its value depends on the data.]
- .smaller[Hence, by chance, it might happen that while the null hypothesis cannot be rejected (supposing it is true), that the p-value is smaller than the set threshold. With the latter chosen as 5%, then, on average, the (sample) p-value is below 5% .purple[once in twenty times!]]
- .smaller[If multiple hypotheses are tested, the chance of observing a rare event increases, and therefore, the chance to incorrectly reject a null hypothesis (i.e., making a Type I error) increases.]
- .smaller[Hence .hi-pink[performing multiple tests, with the same or different data, is dangerous ‚ö†Ô∏è] (but very common! üòü) as it automatically leads to .pink[significant results, when actually there are none!]]

---

# ‚ò†Ô∏è Multiple testing can be dangerous!

&lt;img src="pics/medical_studies2.png" width="70%" style="display: block; margin: auto;" /&gt;

.footnote[.smallest[üëã] Read the original article: "*This is why you shouldn't believe that exciting new medical study*" [here](https://www.vox.com/2015/3/23/8264355/research-study-hype).]

---

# Methods to remedy such problem

- .smallest[The .hi-pink[Bonferroni correction] is one of the oldest methods proposed to counteract the problem of multiple testing (i.e. potential increased chance to make Type I errors). ]
- .smallest[.purple[It tests each individual hypothesis at a reduced significance level.] For example, if a trial is testing 20 hypotheses with a desired] `\(\small \alpha = 5\%\)`.smallest[, then the Bonferroni correction would test each individual hypothesis at] `\(\small \alpha^* = \alpha/20 = 5\% / 20 = 0.25\%\)`.smallest[. Therefore, .purple[it controls the familywise error rate at]] `\(\small \color{#6A5ACD}{\leq \alpha}\)`.
- .smallest[The Bonferroni correction can be .turquoise[conservative] if there are a large number of tests, as it comes at the cost of reducing the power of the tests.]
- .smallest[Another more recent and less conservative method is the .hi-pink[False Discovery Rate (FDR)]. It provides less stringent control of Type I errors compared to the Bonferroni correction. Thus, the FDR has greater power, yet at the cost of increased numbers of Type I errors.]

---

# The multiple-sample location tests

- .smallest[To compare several means of different populations, a standard approach is to start our analysis by using the .pink[multiple-sample location tests]. More precisely, we proceed our analysis with the following steps:]
  - .smallest[.purple[Step 1:] We first perform the multiple-sample location tests, where the null hypothesis states that all the locations are the same. If we cannot reject the null hypothesis, we stop our analysis here. Otherwise, we move on to Step 2.]
  - .smallest[.purple[Step 2:] We compare the groups mutually with two-sample location tests in order to verify our hypothesis.]
- .smallest[For example, suppose we have 3 groups and our hypothesis is that the location of Group A is larger than the location of Group B, which is larger than the location of Group C. We start with a multile-sample location test and see if all the locations are equal. If we reject the null hypothesis, then we perform the two-sample location tests of Group A vs Group B, Group A vs Group C, and Group B vs Group C, which in total are 3 tests.]
- .smallest[.pink[Naturally, we should adjust the significance level for multiple testing in Step 2.]]

---

# The multiple sample location tests

We will discuss three ".pink[multiple-sample location tests]":
1. .purple[Fisher's one-way ANalysis Of VAriance (ANOVA)]
2. .purple[Welch's one-way ANOVA]
3. .purple[Kruskal-Wallis test]

We are interested in comparing means, but why is it an analysis of .purple[variance]?
.pull-left[
&lt;img src="Lecture2_files/figure-html/unnamed-chunk-7-1.svg" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="Lecture2_files/figure-html/unnamed-chunk-8-1.svg" width="100%" style="display: block; margin: auto;" /&gt;
]

---

# Fisher's one-way ANOVA

.smallest[This test considers the following assumed model for G groups]
`$$\small X_{i(g)} = \color{#eb078e}{\mu_{g}} + \varepsilon_{i(g)} = \mu + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},$$`
.smallest[where] `\(\small g=1,\ldots, G\)`, `\(i=1,...,n_{g}\)`, `\(\small \varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma^{2}})\)` .smallest[and] `\(\small \sum n_{g}\delta_{g}=0\)`. 

üìù `\(\small n_i =\)` .smallest[sample size of group] `\(\small i\)`, `\(\small \mu_i = \mu + \delta_i =\)` .smallest[population mean of group] `\(\small i\)`, `\(\small i=1,\ldots, G\)`.

.smallest[Hypotheses:]
`$$\small H_0: \color{#6A5ACD}{\mu_1} \color{#eb078e}{=} \color{#06bcf1}{\mu_2} \color{#eb078e}{=} \ldots \color{#eb078e}{=} \color{#8bb174}{\mu_G} \ \ \ \ \text{and} \ \ \ \ H_a: \mu_i \color{#eb078e}{\neq} \mu_j \ \ \text{for at least one pair of} \ \ (i,j).$$`

.smallest[Test statistic's distribution under] `\(\small H_0\)`:
`$$\small \color{#b4b4b4}{F = \frac{N s^2_{\overline{Y}}}{s_p^2} \ {\underset{H_0}{\sim}} \ \text{Fisher}(G-1, N-G),}$$`
.smallest[.grey[where]] `\(\small \color{#b4b4b4}{s^2_{\overline{Y}} = \frac{1}{G-1} \sum_{g=1}^G \frac{n_g}{N} (\overline{Y}_g - \overline{Y})^2}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{s_p^2 = \frac{1}{N-G} \sum_{g=1}^G (n_g-1)s_g^2}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{N = \sum_{g=1}^G n_g}\)`.smallest[.grey[, and]] `\(\small \color{#b4b4b4}{\overline{Y} = \frac{1}{N} \sum_{g=1}^G n_g \overline{Y}_g}\)`.smallest[.grey[.]]

---

# Discussion - Fisher's one-way ANOVA

- R function: 

.center[
`aov(response ~ groups)`.]

- This test strongly relies on the .pink[assumed absence of outliers]. If outliers appear to be present the Kruskal-Wallis Test (see later) is (probably) a better option.
- For moderate and small sample sizes, the sample distribution should be at least .pink[approximately normal] with no strong skewness to ensure the reliability of the test.
- In practice, the assumption of equal variance is hard to verify and .hi.pink[we recommend avoiding this test in practice].

---

# Welch's one-way ANOVA

.smallest[This test considers the following assumed model for G groups]
`$$\small X_{i(g)} = \color{#eb078e}{\mu_{g}} + \varepsilon_{i(g)} = \mu + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},$$`
.smallest[where] `\(\small g=1,\ldots, G\)`, `\(i=1,...,n_{g}\)`, `\(\small \varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma_g^{2}})\)` .smallest[and] `\(\small \sum n_{g}\delta_{g}=0\)`. 

.smallest[Hypotheses:]
`$$\small H_0: \color{#6A5ACD}{\mu_1} \color{#eb078e}{=} \color{#06bcf1}{\mu_2} \color{#eb078e}{=} \ldots \color{#eb078e}{=} \color{#8bb174}{\mu_G} \ \ \ \ \text{and} \ \ \ \ H_a: \mu_i \color{#eb078e}{\neq} \mu_j \ \ \text{for at least one pair of} \ \ (i,j).$$`

.smallest[Test statistic's distribution under] `\(\small H_0\)`:
`$$\small \color{#b4b4b4}{F = \frac{s^{*^2}_{\overline{Y}}}{1+\frac{2(G-2)}{3\Delta}} \ {\underset{H_0}{\sim}} \ \text{Fisher}(G-1, \Delta),}$$`
.smallest[.grey[where]] `\(\small \color{#b4b4b4}{s^{*^2}_{\overline{Y}} = \frac{1}{G-1} \sum_{g=1}^G w_g (\overline{Y}_g - \overline{Y}^*)^2}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{\Delta = [\frac{3}{G^2-1} \sum_{g=1}^G \frac{1}{n_g} (1-\frac{w_g}{\sum_{g=1}^G w_g})]^{-1}}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{w_g = \frac{n_g}{s_g^2}}\)`.smallest[.grey[, and]] `\(\small \color{#b4b4b4}{\overline{Y}^* = \sum_{g=1}^G \frac{w_g\overline{Y}_g}{\sum_{g=1}^G w_g}}\)`.smallest[.grey[.]]

---

# Discussion - Welch's one-way ANOVA

- R function: 

.center[
`aov(response ~ groups)`.]

- This test strongly relies on the .pink[assumed absence of outliers]. If outliers appear to be present the Kruskal-Wallis Test (see later) is (probably) a better option.
- For moderate and small sample sizes, the sample distribution should be at least .pink[approximately normal] with no strong skewness to ensure the reliability of the test.
- This test does not require the variances of the groups to be equal. If the variances of all the groups are the same (which is rather unlikely in practice), the Welch's one-way ANOVA losses a little bit of power compared to the Fisher's one-way ANOVA.

---

# Kruskal-Wallis test

.smallest[This test considers the following assumed model for G groups]
`$$\small X_{i(g)} = \color{#eb078e}{\theta_{g}} + \varepsilon_{i(g)} = \theta + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},$$`
.smallest[where] `\(\small g=1,\ldots, G\)`, `\(i=1,...,n_{g}\)`, `\(\small \varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma^{2}})\)` .smallest[and] `\(\small \sum n_{g}\delta_{g}=0\)`. 

üìù `\(\small n_i =\)` .smallest[sample size of group] `\(\small i\)`, `\(\small \theta_i = \theta + \delta_i =\)` .smallest[population .pink[location] of group] `\(\small i\)`, `\(\small i=1,\ldots, G\)`.

.smallest[Hypotheses:]
`$$\small H_0: \color{#6A5ACD}{\theta_1} \color{#eb078e}{=} \color{#06bcf1}{\theta_2} \color{#eb078e}{=} \ldots \color{#eb078e}{=} \color{#8bb174}{\theta_G} \ \ \ \ \text{and} \ \ \ \ H_a: \theta_i \color{#eb078e}{\neq} \theta_j \ \ \text{for at least one pair of} \ \ (i,j).$$`

---

# Kruskal-Wallis test

.smallest[Test statistic's distribution under] `\(\small H_0\)`:
`$$\small \color{#b4b4b4}{H = \frac{\frac{12}{N(N+1)} \sum_{g=1}^G \frac{\overline{R}_g}{n_g}-3(N-1)}{1-\frac{\sum_{v=1}^V{t_v^3-t_v}}{N^3-N}} \ {\underset{H_0}{\sim}} \ \mathcal{X}(G-1)}$$`

.smallest[.grey[where]] `\(\small \color{#b4b4b4}{\overline{R}_g = \frac{1}{n_g} \sum_{i=1}^{n_g} R_{i(g)}}\)` .smallest[.grey[with]] `\(\small \color{#b4b4b4}{R_{i(g)}}\)` .smallest[.grey[denoting the global rank of the]] `\(\small \color{#b4b4b4}{i^{th}}\)` .smallest[.grey[observation of group]] `\(\small \color{#b4b4b4}{g}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{V}\)` .smallest[.grey[is the number of different values/levels in]] `\(\small \color{#b4b4b4}{y}\)` .smallest[.grey[and]] `\(\small \color{#b4b4b4}{t_v}\)` .smallest[.grey[denotes the number of times a given value/level occurred in]] `\(\small \color{#b4b4b4}{y}\)`.smallest[.grey[.]]

---

# Discussion - Kruskal-Wallis test

- R function: 

.center[
`kruskal.test(response ~ groups)`.]

- This test is ".pink[robust]" in the sense that (unlike the one-way ANOVA) it is not overly affected by outliers.
- The Kruskal-Wallis test does not require the distributions to be Gaussian nor the variances to be the same, however, in this case this method actually tests whether all distributions are the same. To render the Kruskal-Wallis test comparable to the one-way ANOVA we need to assume: (1) The distribution is symmetric, (2) the variances are the same. Then, we have `\(\theta_i = \mu_i, i=1,\ldots,G\)`.
- Compared to the one-way ANOVA, the Kruskal-Wallis test is less power if their requirements (Gaussian and possibly same variances) are met.

---

# Comparing Diet A, B and C

.panelset[
.panel[.panel-name[Graph]
STEF :)
]
.panel[.panel-name[Import]

```r
# Import data
diet = read.csv("data/diet.csv",row.names=1)

# Compute weight loss
diet$weight.loss = diet$initial.weight - diet$final.weight

# Select diet
posA = diet$diet.type=="A"
posB = diet$diet.type=="B"
posC = diet$diet.type=="C"

# Variable of interest
dietA = diet$weight.loss[posA]
dietB = diet$weight.loss[posB]
dietC = diet$weight.loss[posC]
```
]
.panel[.panel-name[Fisher]

```r
# Number of observations
numA = length(dietA)
numB = length(dietB)
numC = length(dietC)

# Combine 
response = c(dietA, dietB, dietC)
groups = c(rep("A", numA), rep("B", numB), rep("C", numC))

res_aov = aov(response ~ groups)
summary(res_aov)
```

```
#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)   
#&gt; groups       2   60.5  30.264   5.383 0.0066 **
#&gt; Residuals   73  410.4   5.622                  
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]
.panel[.panel-name[Welch]

```r
res_aov = aov(response ~ groups)
summary(res_aov)
```

```
#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)   
#&gt; groups       2   60.5  30.264   5.383 0.0066 **
#&gt; Residuals   73  410.4   5.622                  
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]
.panel[.panel-name[Kruskal]

```r
kruskal.test(response ~ groups)
```

```
#&gt; 
#&gt; 	Kruskal-Wallis rank sum test
#&gt; 
#&gt; data:  response by groups
#&gt; Kruskal-Wallis chi-squared = 9.4159, df = 2, p-value = 0.009023
```
]
.panel[.panel-name[Results]
1. .purple[Define hypotheses:] `\(H_0: \mu_A = \mu_B = \mu_C\)` and `\(H_a: H_0 \ \text{is false}\)`.
2. .purple[Define] `\(\color{#373895}{\alpha}\)`: We consider `\(\alpha = 5\%\)`.
3. .purple[Compute p-value]: All tests show p-values smaller than `\(\alpha\)` (see R output tab for details).
4. .purple[Conclusion:] We have p-value &lt; `\(\alpha\)` so we reject the null hypothesis at the significance level of 5%.
]
.panel[.panel-name[Comp]

```r
wilcox.test(dietA, dietB, alternative = "less")
wilcox.test(dietA, dietC, alternative = "less")
wilcox.test(dietB, dietC, alternative = "less")
```
]
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
